{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "web scraping empresas contacto Chile.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "0DHWeWZc9E94"
      ],
      "mount_file_id": "1keEmQ4Y1o2NkMrrUsyBH6UMylW1sqK1D",
      "authorship_tag": "ABX9TyNhwOAh2VMRt5ommNrsyIKy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/freddyacuna/webscraping-contacto-empresas-chilenas/blob/main/web_scraping_empresas_contacto_Chile.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YJV3KXkSUNb7"
      },
      "source": [
        "# Introducción\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yQH-yWFqY54m"
      },
      "source": [
        "Objetivo: este proyecto consiste en extraer información de contacto (nombre, telefono, mail de contacto) de empresas que se encuentran registradas en SII. Se utiliza python para su ejecución.\n",
        "\n",
        "El código automatiza la búsqueda masiva de la información. Los pasos que deberá tener la busqueda son:\n",
        "\n",
        "\n",
        "\n",
        "1.   Abrir una pestaña del buscador\n",
        "2.   Ir a la pagina de google maps\n",
        "3.   Ingresar el nombre de la empresas que aparece en nuestra bbdd del SII (filtrada para empresas excluye instituciones publicas y personas naturales).\n",
        "4.   Abrir la coincidencia del buscador de google maps.\n",
        "5.   Extraer los datos (nonmbre, telefono, sitio web) y guardarlos en un googlesheet.\n",
        "6.   Si existe sitio web y se logra una coincidencia entre la busqueda de maps y el nombre de la empresa, procedemos a re-direccionar al sitio web.\n",
        "7.   Realizar escrapeo del sitio web para extraer el mail contacto de al menos 15 pestañas del sitio. Luego se guarda en googlesheet los mail de contactos \n",
        "8.   Repetir el procedimiento N donde N corresponde al total de empresas registradas en el SII luego de filtrada la bbdd.\n",
        "\n",
        "\n",
        "Por otro lado, será interesante replicar este mismo procedimiento usando R. Los paquetes relevantes para su aplicación son:\n",
        "\n",
        "\n",
        "\n",
        "*   Realizar web scraping mediante el paquete [rvest] y [polite].\n",
        "*   Acceder y modificar información en planillas de Google Sheets mediante el [googlesheets4].\n",
        "*   Limpiar y modificar caracteres mediante expresiones regulares ([stringr])\n",
        "\n",
        "\n",
        "Si le interesa seguir este camino, no dude en contactarse con: \n",
        "1. [Github Freddy Acuña](https://github.com/freddyacuna)\n",
        "2. [Linkedin Freddy Acuña](https://www.linkedin.com/in/freddyacuna/)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uCYry57-KyIb"
      },
      "source": [
        "Conectamos con google colab y drive para acceder a los archivos de mi nube"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hv2nX4alkLg0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "81e29b30-7dd5-4288-e0b9-8c3edcb83d0a"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0DHWeWZc9E94"
      },
      "source": [
        "## Conectando con googlesheet (**learning**)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NGDiTqSuLEta"
      },
      "source": [
        "Almacenaremos la información de scrapeo en un googlesheet. Una forma inicial de como trabajar con hojas de cálculo de google es la siguiente:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jfbLBJXh9ZPr"
      },
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "###############################################\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "###############################################\n",
        "import gspread\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "gc = gspread.authorize(GoogleCredentials.get_application_default())\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z-obJNX950gB"
      },
      "source": [
        "wb = gc.open(\"AB_NYC_2019\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uu09-q9U6AyK"
      },
      "source": [
        "sheet = wb.worksheet('AB_NYC_2019')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PI8b5ubSNt3v"
      },
      "source": [
        "## Re-conectar (**learning**)\n",
        "\n",
        "Existe la posibilidad de no desconectar el ambiente para que el codigo no se detenga. Con el siguiente métoodo se podrá dar solución [Re-conectar](https://stackoverflow.com/questions/57113226/how-to-prevent-google-colab-from-disconnecting)\n",
        "\n",
        "Deberá ir a : Inspeccionar> Consola > Ingresar al menos uno de los dos codigos que siguen."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5EDCevjJNzLD"
      },
      "source": [
        "function ClickConnect(){\n",
        "console.log(\"Working\"); \n",
        "document.querySelector(\"colab-toolbar-button#connect\").click() \n",
        "}\n",
        "setInterval(ClickConnect,60000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dEgLxK5RZgeu"
      },
      "source": [
        "function ClickConnect(){\n",
        "    console.log(\"Clicked on connect button\"); \n",
        "    document.querySelector(\"Put ID here\").click() // Change id here\n",
        "}\n",
        "setInterval(ClickConnect,60000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x6aySXy1WdTZ"
      },
      "source": [
        "# Web Scraping usando : Selenium + Beautiful Soup "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0DjxIT6n0jK-"
      },
      "source": [
        "Utilizamos dos librerias para la extración de web scraping. \n",
        "\n",
        "1) [Selenium - 1](https://selenium-python.readthedocs.io/) [Selenium 2](https://pypi.org/project/selenium/): Se usa para navegar por los sitios. Abrir pestaña, seleccionar los codigos relevantes en el sitio de google maps para acceder a los items relevantes (nombre, telefono, mail, sitio web)\n",
        "\n",
        "2) [beautiful soup](https://pypi.org/project/beautifulsoup4/): Se usa para navegar en el sitio web re-direccionado para escrapear los mail de contactos.\n",
        "\n",
        "\n",
        "Algunas paginas de referencia que guiaron el camino usando estas librerias en python:\n",
        "\n",
        "\n",
        "https://medium.com/@rodrigonader/web-scraping-to-extract-contact-information-part-1-mailing-lists-854e8a8844d2\n",
        "\n",
        "https://medium.com/nerd-for-tech/linked-in-web-scraper-using-selenium-15189959b3ba\n",
        "\n",
        "https://gist.github.com/lobstrio/b95408a2ed7d01a64e704c417aab64f3\n",
        "\n",
        "https://www.linkedin.com/pulse/how-easy-scraping-data-from-linkedin-profiles-david-craven/\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K70Fv0X-_MXD"
      },
      "source": [
        "## Librerias"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oj9pv2dYAJdo"
      },
      "source": [
        "import os\n",
        "os.chdir('/content/drive/MyDrive/Actual/INNOVA CHILE - CORFO/BBDD/Portafolio/Web Scraping/data')\n",
        "#os.getcwd()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AwwO0WJI_LYL",
        "outputId": "4ef158b5-df1e-4877-fe4e-bfb1836e7be1"
      },
      "source": [
        "!pip install selenium\n",
        "!pip install pyxlsb\n",
        "!apt-get update # to update ubuntu to correctly run apt install\n",
        "!apt install chromium-chromedriver\n",
        "\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "\n",
        "import gspread ## PERMITE CONECTAR CON EL API DE GOOGLE\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "import pyxlsb\n",
        "import time # Lo utilizaremos para que no detecte que somos un bot\n",
        "import random\n",
        "\n",
        "from datetime import datetime\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.common.action_chains import ActionChains\n",
        "import sys\n",
        "sys.path.insert(0,'/usr/lib/chromium-browser/chromedriver')\n",
        "\n",
        "import re                           #expresiones regulares\n",
        "import requests                     #para enviar solicitudes HTTP\n",
        "from urllib.parse import urlsplit   #dividir las URL en partes componentes\n",
        "from collections import deque       #contenedor similar a una lista con anexos \n",
        "from bs4 import BeautifulSoup       #extraer datos de archivos HTML de sitios web\n",
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting selenium\n",
            "  Downloading selenium-3.141.0-py2.py3-none-any.whl (904 kB)\n",
            "\u001b[K     |████████████████████████████████| 904 kB 6.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: urllib3 in /usr/local/lib/python3.7/dist-packages (from selenium) (1.24.3)\n",
            "Installing collected packages: selenium\n",
            "Successfully installed selenium-3.141.0\n",
            "Collecting pyxlsb\n",
            "  Downloading pyxlsb-1.0.8-py2.py3-none-any.whl (23 kB)\n",
            "Installing collected packages: pyxlsb\n",
            "Successfully installed pyxlsb-1.0.8\n",
            "Get:1 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease [3,626 B]\n",
            "Ign:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "Ign:3 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "Get:4 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release [697 B]\n",
            "Hit:5 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "Get:6 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release.gpg [836 B]\n",
            "Get:7 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n",
            "Get:8 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease [15.9 kB]\n",
            "Get:9 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ Packages [62.9 kB]\n",
            "Hit:10 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "Get:11 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n",
            "Hit:13 http://ppa.launchpad.net/cran/libgit2/ubuntu bionic InRelease\n",
            "Ign:14 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Packages\n",
            "Get:14 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Packages [695 kB]\n",
            "Get:15 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]\n",
            "Hit:16 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic InRelease\n",
            "Get:17 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease [21.3 kB]\n",
            "Get:18 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [2,263 kB]\n",
            "Get:19 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main Sources [1,786 kB]\n",
            "Get:20 http://security.ubuntu.com/ubuntu bionic-security/multiverse amd64 Packages [26.7 kB]\n",
            "Get:21 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [1,420 kB]\n",
            "Get:22 http://security.ubuntu.com/ubuntu bionic-security/restricted amd64 Packages [510 kB]\n",
            "Get:23 http://archive.ubuntu.com/ubuntu bionic-updates/multiverse amd64 Packages [39.4 kB]\n",
            "Get:24 http://archive.ubuntu.com/ubuntu bionic-updates/restricted amd64 Packages [544 kB]\n",
            "Get:25 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [2,698 kB]\n",
            "Get:26 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main amd64 Packages [914 kB]\n",
            "Get:27 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [2,195 kB]\n",
            "Get:28 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic/main amd64 Packages [44.1 kB]\n",
            "Fetched 13.5 MB in 4s (3,087 kB/s)\n",
            "Reading package lists... Done\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  chromium-browser chromium-browser-l10n chromium-codecs-ffmpeg-extra\n",
            "Suggested packages:\n",
            "  webaccounts-chromium-extension unity-chromium-extension\n",
            "The following NEW packages will be installed:\n",
            "  chromium-browser chromium-browser-l10n chromium-chromedriver\n",
            "  chromium-codecs-ffmpeg-extra\n",
            "0 upgraded, 4 newly installed, 0 to remove and 94 not upgraded.\n",
            "Need to get 86.0 MB of archives.\n",
            "After this operation, 298 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 chromium-codecs-ffmpeg-extra amd64 91.0.4472.101-0ubuntu0.18.04.1 [1,124 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 chromium-browser amd64 91.0.4472.101-0ubuntu0.18.04.1 [76.1 MB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 chromium-browser-l10n all 91.0.4472.101-0ubuntu0.18.04.1 [3,937 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 chromium-chromedriver amd64 91.0.4472.101-0ubuntu0.18.04.1 [4,837 kB]\n",
            "Fetched 86.0 MB in 7s (12.7 MB/s)\n",
            "Selecting previously unselected package chromium-codecs-ffmpeg-extra.\n",
            "(Reading database ... 160837 files and directories currently installed.)\n",
            "Preparing to unpack .../chromium-codecs-ffmpeg-extra_91.0.4472.101-0ubuntu0.18.04.1_amd64.deb ...\n",
            "Unpacking chromium-codecs-ffmpeg-extra (91.0.4472.101-0ubuntu0.18.04.1) ...\n",
            "Selecting previously unselected package chromium-browser.\n",
            "Preparing to unpack .../chromium-browser_91.0.4472.101-0ubuntu0.18.04.1_amd64.deb ...\n",
            "Unpacking chromium-browser (91.0.4472.101-0ubuntu0.18.04.1) ...\n",
            "Selecting previously unselected package chromium-browser-l10n.\n",
            "Preparing to unpack .../chromium-browser-l10n_91.0.4472.101-0ubuntu0.18.04.1_all.deb ...\n",
            "Unpacking chromium-browser-l10n (91.0.4472.101-0ubuntu0.18.04.1) ...\n",
            "Selecting previously unselected package chromium-chromedriver.\n",
            "Preparing to unpack .../chromium-chromedriver_91.0.4472.101-0ubuntu0.18.04.1_amd64.deb ...\n",
            "Unpacking chromium-chromedriver (91.0.4472.101-0ubuntu0.18.04.1) ...\n",
            "Setting up chromium-codecs-ffmpeg-extra (91.0.4472.101-0ubuntu0.18.04.1) ...\n",
            "Setting up chromium-browser (91.0.4472.101-0ubuntu0.18.04.1) ...\n",
            "update-alternatives: using /usr/bin/chromium-browser to provide /usr/bin/x-www-browser (x-www-browser) in auto mode\n",
            "update-alternatives: using /usr/bin/chromium-browser to provide /usr/bin/gnome-www-browser (gnome-www-browser) in auto mode\n",
            "Setting up chromium-chromedriver (91.0.4472.101-0ubuntu0.18.04.1) ...\n",
            "Setting up chromium-browser-l10n (91.0.4472.101-0ubuntu0.18.04.1) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Processing triggers for hicolor-icon-theme (0.17-2) ...\n",
            "Processing triggers for mime-support (3.60ubuntu1) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1.2) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/python3.7/dist-packages/ideep4py/lib/libmkldnn.so.0 is not a symbolic link\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yisvfYxmpvC7"
      },
      "source": [
        "## Code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "js8Zq9Vqp-tb"
      },
      "source": [
        "class ScrapearGMaps:\n",
        "    \n",
        "    data = {} # Info que extreré de las empresas\n",
        "    worksheet = {}\n",
        "    \n",
        "    def __init__(self):\n",
        "        options = webdriver.ChromeOptions()\n",
        "        options.add_argument('--headless')\n",
        "        options.add_argument('--no-sandbox')\n",
        "        options.add_argument('--disable-dev-shm-usage')\n",
        "        self.driver = webdriver.Chrome('chromedriver',options=options)\n",
        "        \n",
        "        now = datetime.now()\n",
        "        today = now.strftime(\"%Y-%m-%d\")\n",
        "\n",
        "        gc = gspread.authorize(GoogleCredentials.get_application_default())      # Conectamos con nuestra cuenta de google drive\n",
        "\n",
        "        # Abrir por titulo\n",
        "        sh = gc.open(\"Empresas Contacto\")\n",
        "        #sh = gc.open_by_url('https://docs.google.com/spreadsheets/d/1Gvdp_0QDu5wy683i0vx9dfeQo1GoTX-eC3uPLbAEDr8/edit?usp=sharing')\n",
        "  \n",
        "\n",
        "        # SSeleccionar primera hoja para ir añandiendo las cosas\n",
        "        self.worksheet = sh.get_worksheet(0)\n",
        "        #self.worksheet = sh.worksheet('bbdd')\n",
        "    \n",
        "\n",
        "    # Extraera Nombre de la empresa    \n",
        "    def get_name(self):\n",
        "        try:\n",
        "            return self.driver.find_element_by_xpath(\"//h1[contains(@class,'header-title')]\").text\n",
        "        except:\n",
        "            return \"\"\n",
        "\n",
        "\n",
        "    # Extraera telefono de la empresa     \n",
        "    def get_phone(self):\n",
        "        try:\n",
        "            return self.driver.find_element_by_xpath(\"//button[contains(@data-item-id,'phone')]\").text\n",
        "        except:\n",
        "            return \"\"\n",
        "\n",
        "    # Extraera sitio web de la empresa     \n",
        "    def get_website(self):\n",
        "        try:\n",
        "            return self.driver.find_element_by_css_selector(\"[data-item-id='authority']\").text\n",
        "        except:\n",
        "            return \"\"\n",
        "\n",
        "##      M     A     I     L\n",
        "\n",
        "    # Extraera mail de la empresa    \n",
        "    def get_email(self, url_website):\n",
        "        try:\n",
        "            starting_url = url_website\n",
        "            unprocessed_urls = deque([starting_url])\n",
        "            processed_urls = set()\n",
        "            emails = set()\n",
        "\n",
        "            while len(unprocessed_urls):\n",
        "                if len(processed_urls)>15:      # limite para evitar las redirecciones en sitios web: e-commerce + otras páginas\n",
        "                      break\n",
        "                url = unprocessed_urls.popleft()\n",
        "                processed_urls.add(url)\n",
        "                parts = urlsplit(url)\n",
        "                base_url = \"{0.scheme}://{0.netloc}\".format(parts)\n",
        "                path = url[:url.rfind('/')+1] if '/' in parts.path else url\n",
        "\n",
        "                #print(\"Crawling URL %s\" % url)\n",
        "                try:\n",
        "                    response = requests.get(url)\n",
        "                except (requests.exceptions.MissingSchema, requests.exceptions.ConnectionError):\n",
        "                    continue\n",
        "                \n",
        "                new_emails = set(re.findall(r\"[a-z0-9\\.\\-+_]+@[a-z0-9\\.\\-+_]+\\.[cl|com]+\", response.text, re.I)) #[. In | .com | .uk] @ + \\ S +\n",
        "                emails.update(new_emails)\n",
        "                #print(emails)\n",
        "\n",
        "                soup = BeautifulSoup(response.text, 'lxml')\n",
        "\n",
        "                for anchor in soup.find_all(\"a\"):\n",
        "                    link = anchor.attrs[\"href\"] if \"href\" in anchor.attrs else ''\n",
        "                    if link.startswith('/'):\n",
        "                        link = base_url + link\n",
        "                    elif not link.startswith('http'):\n",
        "                        link = path + link\n",
        "                    if not link in unprocessed_urls and not link in processed_urls:\n",
        "                        unprocessed_urls.append(link)\n",
        "                    \n",
        "\n",
        "\n",
        "\n",
        "            return ','.join(emails)   # (contacto@chile.cl, contacto2 , )\n",
        "            \n",
        "        except:\n",
        "            return \"\"\n",
        "    \n",
        "    def scrape(self, url):\n",
        "        try:\n",
        "            self.driver.get(url)        ## Accedemos a la URL\n",
        "            #print(self.driver.page_source)\n",
        "            time.sleep(1)\n",
        "            \n",
        "            #element = self.driver.find_element_by_xpath(\"//button[.//span[text()='Acepto']]\")        ## solucion al problemas de cookies si estamos trabajando en un entorno local. Es una ruta relativa y no absoluta\n",
        "            #element.click()\n",
        "            \n",
        "            time.sleep(1)\n",
        "                        \n",
        "      \n",
        "            name = self.get_name()\n",
        "            address = self.get_address()\n",
        "            phone_number = self.get_phone()\n",
        "            website = self.get_website()\n",
        "            coords = self.get_geocoder(self.driver.current_url)\n",
        "            email = \"\"\n",
        "            if website != \"\":\n",
        "                email = self.get_email('http://'+website)\n",
        "                \n",
        "                \n",
        "            #print([name, address, phone_number, coords[0], coords[1], website, email])\n",
        "                \n",
        "                \n",
        "            row_index = len(self.worksheet.col_values(1)) + 1\n",
        "            self.worksheet.update_acell('A'+str(row_index), name)\n",
        "            self.worksheet.update_acell('B'+str(row_index), '#'+phone_number)\n",
        "            self.worksheet.update_acell('C'+str(row_index), website)\n",
        "            self.worksheet.update_acell('D'+str(row_index), email)\n",
        "            self.worksheet.update_acell('E'+str(row_index), query)                  \n",
        "                \n",
        " \n",
        "            time.sleep(1)\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(e)\n",
        "        \n",
        "        time.sleep(3)\n",
        "        #self.driver.quit()\n",
        "\n",
        "        return(self.data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r9LMSmu-FjPV"
      },
      "source": [
        "## Consulta"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dExY7IJ5pNOW"
      },
      "source": [
        "### BBDD empresas completas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZjJdF8zpi5v_"
      },
      "source": [
        "Cargamos la bbdd empresas sii campo \"tipo de contribuyente\"."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qBy_cXl6fk8Y"
      },
      "source": [
        "df = pd.read_excel('/content/drive/MyDrive/Actual/INNOVA CHILE - CORFO/BBDD/Portafolio/Web Scraping/data/PUB_Empresas_2019_102020.xlsb', engine='pyxlsb')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DfIYNHjshGWj",
        "outputId": "3ad1556d-f31e-42af-d0d1-defffd8b190f"
      },
      "source": [
        "df.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(673847, 20)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QB655YI7pTYA"
      },
      "source": [
        "### BBDD empresas filtrado"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k1-u9gT-pWij"
      },
      "source": [
        "Se limpia por termino de giros y solo se mantienen empresas juridicas"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a9f8MtHMpeMo"
      },
      "source": [
        "df = pd.read_excel('/content/drive/MyDrive/Actual/INNOVA CHILE - CORFO/BBDD/Portafolio/Web Scraping/data/PUB_Empresas_2019_102020_personas_juridicas.xlsb', engine='pyxlsb')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QR-oRnKWpeMp",
        "outputId": "bbf20db6-aba6-4ce8-882e-5f9f23933abb"
      },
      "source": [
        "df.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(636696, 20)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wqKXB5uJkpcM"
      },
      "source": [
        "### Busqueda Razón Social (**learning**)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "5bhSuhKiLKHZ",
        "outputId": "79969f34-0bf4-4515-9dd9-1003aad559fe"
      },
      "source": [
        "name = df['Razón social'][0]\n",
        "name"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'CENTRO DE MATERIALES DE CONSTRUCCION FERNANDEZ S A'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jl7oHmFPx9Hp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6e0bb767-2f81-4717-a06a-e34993be024d"
      },
      "source": [
        "query = name\n",
        "url = \"https://www.google.cl/maps?q=\"+query.replace(\" \", \"+\")+\"&hl=es\"\n",
        "print(url)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "https://www.google.cl/maps?q=CENTRO+DE+MATERIALES+DE+CONSTRUCCION+FERNANDEZ+S+A&hl=es\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AL74YYsovWyU"
      },
      "source": [
        "Obs: \n",
        "\n",
        "1) empresas no cuentan con sitio web.\n",
        "\n",
        "\n",
        "2) si exiten más enlaces (+1) sol:\n",
        "\n",
        "3) empresas no existen en la busqueda de google maps"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l1xsb8KHrBrN",
        "outputId": "04747dd3-c94e-4ad0-964a-89d66965770a"
      },
      "source": [
        "for i in range(0,3):\n",
        "  print(i)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n",
            "1\n",
            "2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iwLuVDB9y02K"
      },
      "source": [
        "### Tramos"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CNemoModK3oL"
      },
      "source": [
        " for i in range(0,636696):\n",
        "   query = df['Razón social'][i]\n",
        "   url = \"https://www.google.cl/maps?q=\"+query.replace(\" \", \"+\")+\"&hl=es\"\n",
        "   gmaps = ScrapearGMaps()\n",
        "   gmaps.scrape(url)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}